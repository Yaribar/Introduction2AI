{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment\n",
    "\n",
    "Build a 2-layer MLP for MNIST digit classfication. Feel free to play around with the model architecture and see how the training time/performance changes, but to begin, try the following:\n",
    "\n",
    "Image (784 dimensions) ->  \n",
    "fully connected layer (500 hidden units) -> nonlinearity (ReLU) ->  \n",
    "fully connected (10 hidden units) -> softmax\n",
    "\n",
    "Try building the model both with basic PyTorch operations, and then again with more object-oriented higher-level APIs. \n",
    "You should get similar results!\n",
    "\n",
    "\n",
    "*Some hints*:\n",
    "- Even as we add additional layers, we still only require a single optimizer to learn the parameters.\n",
    "Just make sure to pass all parameters to it!\n",
    "- As you'll calculate in the Short Answer, this MLP model has many more parameters than the logisitic regression example, which makes it more challenging to learn.\n",
    "To get the best performance, you may want to play with the learning rate and increase the number of training epochs.\n",
    "- Be careful using `torch.nn.CrossEntropyLoss()`. \n",
    "If you look at the [PyTorch documentation](https://pytorch.org/docs/stable/nn.html#crossentropyloss): you'll see that `torch.nn.CrossEntropyLoss()` combines the softmax operation with the cross-entropy.\n",
    "This means you need to pass in the logits (predictions pre-softmax) to this loss.\n",
    "Computing the softmax separately and feeding the result into `torch.nn.CrossEntropyLoss()` will significantly degrade your model's performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Check if MPS (Apple Silicon GPU) is available\n",
    "#if torch.backends.mps.is_available():\n",
    "#    device = torch.device('mps')\n",
    "#elif torch.cuda.is_available():\n",
    "#    device = torch.device('cuda')\n",
    "#else:\n",
    "device = torch.device('cpu')\n",
    "\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Load the data\n",
    "mnist_train = datasets.MNIST(root=\"./datasets\", train=True, transform=transforms.ToTensor(), download=True)\n",
    "mnist_test = datasets.MNIST(root=\"./datasets\", train=False, transform=transforms.ToTensor(), download=True)\n",
    "train_loader = torch.utils.data.DataLoader(mnist_train, batch_size=100, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(mnist_test, batch_size=100, shuffle=False)\n",
    "\n",
    "## Training\n",
    "# Initialize parameters\n",
    "W = torch.randn(784, 500, device=device) / np.sqrt(784)\n",
    "W.requires_grad_()\n",
    "b = torch.zeros(500, device=device, requires_grad=True)\n",
    "\n",
    "W1 = torch.randn(500, 10, device=device) / np.sqrt(500)\n",
    "W1.requires_grad_()\n",
    "b1 = torch.zeros(10, device=device, requires_grad=True)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.SGD([W, b, W1, b1], lr=0.1)\n",
    "\n",
    "# Iterate through train set minibatches\n",
    "for images, labels in tqdm(train_loader):\n",
    "    # Move data to device\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    \n",
    "    # Zero out the gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    x = images.view(-1, 28*28)\n",
    "    y_0 = torch.matmul(x, W) + b\n",
    "    x_relu_F = F.relu(y_0)\n",
    "    y_1 = torch.matmul(x_relu_F, W1) + b1\n",
    "\n",
    "    cross_entropy = F.cross_entropy(y_1, labels)\n",
    "    # Backward pass\n",
    "    cross_entropy.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "## Testing\n",
    "correct = 0\n",
    "total = len(mnist_test)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Iterate through test set minibatches\n",
    "    for images, labels in tqdm(test_loader):\n",
    "        # Move data to device\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        x = images.view(-1, 28*28)\n",
    "        y_0 = torch.matmul(x, W) + b\n",
    "        x_relu_F = F.relu(y_0)\n",
    "        y_1 = torch.matmul(x_relu_F, W1) + b1\n",
    "        \n",
    "        predictions = torch.argmax(y_1, dim=1)\n",
    "        correct += torch.sum((predictions == labels).float())\n",
    "    \n",
    "print('Test accuracy: {}'.format(correct / total))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
